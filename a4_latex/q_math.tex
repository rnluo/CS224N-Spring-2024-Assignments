\graphicspath{ {images/} }

\titledquestion{Attention Exploration}[14]
\label{sec:analysis}

Multi-head self-attention is the core modeling component of Transformers.
In this question, we'll get some practice working with the self-attention equations, and motivate why multi-headed self-attention can be preferable to single-headed self-attention.

Recall that attention can be viewed as an operation on a \textit{query} vector $q\in\mathbb{R}^d$, a set of \textit{value} vectors $\{v_1,\dots,v_n\}, v_i\in\mathbb{R}^d$, and a set of \textit{key} vectors $\{k_1,\dots,k_n\}, k_i \in \mathbb{R}^d$, specified as follows:
\begin{align}
&c = \sum_{i=1}^{n} v_i \alpha_i \\
&\alpha_i = \frac{\exp(k_i^\top q)}{\sum_{j=1}^{n} \exp(k_j^\top q)}
\end{align} 
with $alpha = \{\alpha_1, \ldots, \alpha_n\}$ termed the ``attention weights''. 
Observe that the output $c\in\mathbb{R}^d$ is an average over the value vectors weighted with respect to $\alpha$.

\begin{parts}

\part[3] \label{copying} \textbf{Copying in attention.} One advantage of attention is that it's particularly easy to ``copy'' a value vector to the output $c$. In this problem, we'll motivate why this is the case.

\begin{subparts}
    \subpart[2] The distribution $\alpha$ is typically relatively ``diffuse''; the probability mass is spread out between many different $\alpha_i$. However, this is not always the case. \textbf{Describe} (in one sentence) under what conditions the categorical distribution $\alpha$ puts almost all of its weight on some $\alpha_j$, where $j \in \{1, \ldots, n\}$ (i.e. $\alpha_j \gg \sum_{i \neq j} \alpha_i$). What must be true about the query $q$ and/or the keys $\{k_1,\dots,k_n\}$?

    \ifans{} Query $q \approx k_j$, and the keys $\{k_1, \ldots, k_n\}$ are nearly orthogonal.

    \subpart[1] Under the conditions you gave in (i), \textbf{describe} the output $c$. 

    \ifans{} Given the conditions, the output $c \approx v_j$.

\end{subparts}


\part[2]\textbf{An average of two.} 
\label{q_avg_of_two}
Instead of focusing on just one vector $v_j$, a Transformer model might want to incorporate information from \textit{multiple} source vectors.

Consider the case where we instead want to incorporate information from \textbf{two} vectors $v_a$ and $v_b$, with corresponding key vectors $k_a$ and $k_b$.
Assume that (1) all key vectors are orthogonal, so $k_i^\top k_j = 0$ for all $i \neq j$; and (2) all key vectors have norm $1$.
\textbf{Find an expression} for a query vector $q$ such that $c \approx \frac{1}{2}(v_a + v_b)$, and \textbf{justify your answer}.\footnote{Hint: while the softmax function will never \textit{exactly} average the two vectors, you can get close by using a large scalar multiple in the expression.} (Recall what you learned in part~(\ref{copying}).)

\ifans{} Let $q = \lambda(k_a + k_b)$, where $\lambda$ is a large positive scalar. Then $k_j^\top q = 0$ for $k_j \in \{k_1, \ldots, k_n\} \setminus \{k_a, k_b\}$. 
\begin{align*}
    \alpha_a &= \frac{\exp(k_a^\top \lambda (k_a + k_b))}{\sum_{j=1}^{n} \exp(k_j^\top q)} \\
    &= \frac{\exp(\lambda)}{\sum_{j=1}^{n} \exp(k_j^\top q)} \\
    \alpha_b &= \frac{\exp(k_b^\top \lambda (k_a + k_b))}{\sum_{j=1}^{n} \exp(k_j^\top q)} \\
    &= \frac{\exp(\lambda)}{\sum_{j=1}^{n} \exp(k_j^\top q)} \\
    \Rightarrow \alpha_a = \alpha_b &= \frac{\exp(\lambda)}{2 \cdot \exp(\lambda) + n - 2} \approx \frac{1}{2}\\
    \alpha_i &\approx 0 \textrm{ for } i \neq a, b \\
    c &\approx \frac{1}{2} (v_a + v_b)
\end{align*}


\part[5]\textbf{Drawbacks of single-headed attention:} \label{q_problem_with_single_head}
In the previous part, we saw how it was \textit{possible} for a single-headed attention to focus equally on two values.
The same concept could easily be extended to any subset of values.
In this question we'll see why it's not a \textit{practical} solution.

Consider a set of key vectors $\{k_1,\dots,k_n\}$ that are now randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i \in \mathbb{R}^d$ are known to you, but the covariances $\Sigma_i$ are unknown (unless specified otherwise in the question).
Further, assume that the means $\mu_i$ are all perpendicular; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.

\begin{subparts}
\subpart[2] Assume that the covariance matrices are $\Sigma_i = \alpha I, \forall i \in \{1, 2, \ldots, n\}$, for vanishingly small $\alpha$.
Design a query $q$ in terms of the $\mu_i$ such that as before, $c\approx \frac{1}{2}(v_a + v_b)$, and provide a brief argument as to why it works.

\ifans{} Let $q = \lambda(\mu_a + \mu_b)$, where $\lambda$ is a large positive scalar. Then $\mu_j^\top q = 0$ for $j \in \{\mu_1, \ldots, \mu_n\} \setminus \{\mu_a, \mu_b\}$. 
Since $\alpha$ is vanishingly small, we can approximate $k_i \approx \mu_i$.
\begin{align*}
    \alpha_a &= \frac{\exp(\mu_a^\top \lambda (\mu_a + \mu_b))}{\sum_{j=1}^{n} \exp(\mu_j^\top q)} \\
    &= \frac{\exp(\lambda)}{\sum_{j=1}^{n} \exp(\mu_j^\top q)} \\
    \alpha_b &= \frac{\exp(\mu_b^\top \lambda (\mu_a + \mu_b))}{\sum_{j=1}^{n} \exp(\mu_j^\top q)} \\
    &= \frac{\exp(\lambda)}{\sum_{j=1}^{n} \exp(\mu_j^\top q)} \\
    \Rightarrow \alpha_a = \alpha_b &= \frac{\exp(\lambda)}{2 \cdot \exp(\lambda) + n - 2} \approx \frac{1}{2}\\
    \alpha_i &\approx 0 \textrm{ for } i \neq a, b \\
    c &\approx \frac{1}{2} (v_a + v_b)
\end{align*}

\subpart[3] Though single-headed attention is resistant to small perturbations in the keys, some types of larger perturbations may pose a bigger issue. In some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\mu_a$.\footnote{Unlike the original Transformer, newer Transformer models apply layer normalization before attention. In these pre-layernorm models, norms of keys cannot be too different which makes the situation in this question less likely to occur.}

As an example, let us consider a covariance for item $a$ as $\Sigma_a = \alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$ (as shown in figure \ref{ka_plausible}). This causes $k_a$ to point in roughly the same direction as $\mu_a$, but with large variances in magnitude. Further, let $\Sigma_i = \alpha I$ for all $i \neq a$.
\begin{figure}[h]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[width=0.35\linewidth]{images/ka_plausible.png}
\caption{The vector $\mu_a$ (shown here in 2D as an example), with the range of possible values of $k_a$ shown in red. As mentioned previously, $k_a$ points in roughly the same direction as $\mu_a$, but may have larger or smaller magnitude.}
\label{ka_plausible}
\end{figure}

When you sample $\{k_1,\dots,k_n\}$ multiple times, and use the $q$ vector that you defined in part i., what do you expect the vector $c$ will look like qualitatively for different samples? Think about how it differs from part (i) and how $c$'s variance would be affected.

\newpage

\ifans{} When sampling $k_i$ from $\{k_1, \ldots, k_n\}$, similar to part (i), if $i \neq a, b$, we have $k_i^\top q \approx 0$; if $i = b$, we have $k_i^\top q \approx \lambda$.\\
If $i = a$, let $k_a = \mu_a + \epsilon_a$, where $\epsilon_a \sim \mathcal{N}(0, \alpha I + \frac{1}{2}(\mu_a \mu_a^\top))$.
\begin{align*}
    k_a^\top q &= (\mu_a + \epsilon_a)^\top \lambda (\mu_a + \mu_b)\\
    &= \lambda + \lambda (\mu_a + \mu_b)^\top \epsilon_a\\
    \mathbb{E}[k_a^\top q] &= \lambda + \lambda (\mu_a + \mu_b)^\top \mathbb{E}[\epsilon_a]\\
    &= \lambda\\
    \textrm{Var}[k_a^\top q] &= \lambda^2 \textrm{Var}[(\mu_a + \mu_b)^\top \epsilon_a]\\
    &= \lambda^2 (\mu_a + \mu_b) (\alpha I + \frac{1}{2} \mu_a \mu_a^T) (\mu_a + \mu_b)\\
    &= \lambda^2 (\alpha \Vert \mu_a + \mu_b \Vert^2 + \frac{1}{2}((\mu_a + \mu_b)^\top \mu_a)^2)\\
    &= \lambda^2 (2\alpha + \frac{1}{2})\\
    &\approx \frac{\lambda^2}{2} 
\end{align*}
While the mean of $k_a^\top q$ is the same, its variance is considerable as $\lambda$ is a large scalar. This results in an oscillating $\alpha_a$.
If $k_a$ is large, the vector $c \approx v_a$, and vice versa. This prevents proper averaging of the two value vectors.

\end{subparts}

\part[3]\textbf{Benefits of multi-headed attention:} \label{q_multi_head}
Now we'll see some of the power of multi-headed attention.
We'll consider a simple version of multi-headed attention which is identical to single-headed self-attention as we've presented it, except two query vectors ($q_1$ and $q_2$) are defined, which leads to a pair of vectors ($c_1$ and $c_2$), each the output of single-headed attention given its respective query vector.
The final output of the multi-headed attention is their average, $\frac{1}{2}(c_1+c_2)$.

As in question 1(\ref{q_problem_with_single_head}), consider a set of key vectors $\{k_1,\dots,k_n\}$ that are randomly sampled, $k_i\sim \mathcal{N}(\mu_i, \Sigma_i)$, where the means $\mu_i$ are known to you, but the covariances $\Sigma_i$ are unknown.
Also as before, assume that the means $\mu_i$ are mutually orthogonal; $\mu_i^\top \mu_j = 0$ if $i\not=j$, and unit norm, $\|\mu_i\|=1$.

\begin{subparts}
\subpart[1]
Assume that the covariance matrices are $\Sigma_i=\alpha I$, for vanishingly small $\alpha$.
Design $q_1$ and $q_2$ in terms of $\mu_i$ such that $c$ is approximately equal to $\frac{1}{2}(v_a+v_b)$. 
Note that $q_1$ and $q_2$ should have different expressions.

\ifans{} Let $q_1 = \lambda \mu_a$ and $q_2 = \lambda \mu_b$, where $\lambda$ is a large positive scalar.

\subpart[2]
Assume that the covariance matrices are $\Sigma_a=\alpha I + \frac{1}{2}(\mu_a\mu_a^\top)$ for vanishingly small $\alpha$, and $\Sigma_i=\alpha I$  for all $i \neq a$.
Take the query vectors $q_1$ and $q_2$ that you designed in part i.
What, qualitatively, do you expect the output $c$ to look like across different samples of the key vectors? Explain briefly in terms of variance in $c_1$ and $c_2$. You can ignore cases in which $k_a^\top q_i < 0$.

\ifans{} By a similar process to that in question 1(c), we have $\textrm{Var}[k_a^T q_1] \approx \frac{\lambda^2}{2}$, and $\textrm{Var}[k_a^T q_2] \approx 0$. While variance of $c_1$ is high, the vector $c_2$ has consistent low variance. 
If $k_a$ is large, the output $c \approx \frac{1}{2} (v_a + v_b)$. If $k_a$ is small, the output $c \approx \frac{1}{2} v_b$.

\end{subparts}

\part[1] Based on part~(\ref{q_multi_head}), briefly summarize how multi-headed attention overcomes the drawbacks of single-headed attention that you identified in part~(\ref{q_problem_with_single_head}).

\ifans{} Due to the stablizing effect of $c_2$, the output never completely ignore $v_b$. Averaging $c_1$ with the stable $c_2$ also reduces the variance of $c$.

\end{parts}
